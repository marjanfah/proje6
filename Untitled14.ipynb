{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0DRV4_Ap3Zb7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the 'train.csv' file\n",
        "df = pd.read_csv(r\"C:\\Users\\METACO\\Desktop\\train.csv\", sep=\"\\t\")\n",
        "#delimiter='\\t'\n",
        "\n",
        "# Select the 'article' and 'summary' columns\n",
        "df = df[['article', 'summary']]\n",
        "# Print the resulting dataframe\n",
        "#print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iKVDIif53bOY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the 'test.csv' file\n",
        "dt = pd.read_csv(r\"C:\\Users\\METACO\\Desktop\\test.csv\", sep=\"\\t\")\n",
        "#delimiter='\\t'\n",
        "\n",
        "# Select the 'article' and 'summary' columns\n",
        "dt = dt[['article', 'summary']]\n",
        "\n",
        "# Print the resulting dataframe\n",
        "#print(dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H2RVkTSl3bK0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "article    0\n",
            "summary    0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [article, summary]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Print the count of NaN values in each column\n",
        "print(df.isna().sum())\n",
        "\n",
        "# Print the rows containing NaN values\n",
        "print(df[df.isna().any(axis=1)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YMyg7X4w3bIQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "article    0\n",
            "summary    0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [article, summary]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Print the count of NaN values in each column\n",
        "print(dt.isna().sum())\n",
        "\n",
        "# Print the rows containing NaN values\n",
        "print(dt[dt.isna().any(axis=1)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AEWHbc6j3bFr"
      },
      "outputs": [],
      "source": [
        "col_cleandf = df.columns.tolist()\n",
        "col_cleandt = dt.columns.tolist()\n",
        "\n",
        "for column in col_cleandf:\n",
        "    df[column] = df[column].str.replace('', '')\n",
        "for column in col_cleandt:\n",
        "    dt[column] = dt[column].str.replace('', '')\n",
        "\n",
        "    df[column] = df[column].str.replace('[«»\\(\\):]', ' ', regex=True)\n",
        "    dt[column] = dt[column].str.replace('[«»\\(\\):]', ' ', regex=True)\n",
        "\n",
        "    df[column] = df[column].str.replace('\\s+', ' ', regex=True)\n",
        "    dt[column] = dt[column].str.replace('\\s+', ' ', regex=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wSOxHCoz3bCf"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress the FutureWarning\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# PREPROCESSING\n",
        "cleandf = df.columns.tolist()\n",
        "cleandt = dt.columns.tolist()\n",
        "\n",
        "for column in cleandf:\n",
        "    df[column] = df[column].str.replace('[«»\\(\\):]', ' ', regex=True)\n",
        "    df[column] = df[column].str.replace('\\s+', ' ', regex=True)\n",
        "for column in cleandt:\n",
        "    dt[column] = dt[column].str.replace('[«»\\(\\):]', ' ', regex=True)\n",
        "    dt[column] = dt[column].str.replace('\\s+', ' ', regex=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iCc9IQnB3bAC"
      },
      "outputs": [],
      "source": [
        "from hazm import *\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v6y7lHjz3a9d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import hazm\n",
        "\n",
        "# Create an instance of the normalizer\n",
        "normalizer = hazm.Normalizer()\n",
        "\n",
        "# Apply the normalizer to the selected columns\n",
        "df = df.apply(lambda x: x.apply(normalizer.normalize) if isinstance(x, pd.Series) else normalizer.normalize(x) if pd.notnull(x) else x)\n",
        "dt = dt.apply(lambda x: x.apply(normalizer.normalize) if isinstance(x, pd.Series) else normalizer.normalize(x) if pd.notnull(x) else x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4fpHVgRT3a7P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\METACO\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Create a lemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "df = df.apply(lambda x: x.apply(lambda word: lemmatizer.lemmatize(word)))\n",
        "dt = dt.apply(lambda x: x.apply(lambda word: lemmatizer.lemmatize(word)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "v6mLHyLS3a4g"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords\n",
        "stopwords = hazm.stopwords_list()\n",
        "\n",
        "# Remove stopwords from the 'article' column\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "df['article'] = df['article'].apply(remove_stopwords)\n",
        "\n",
        "# Remove stopwords from the 'summary' column\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "df['summary'] = df['summary'].apply(remove_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sQizDpnO3a2g"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords\n",
        "stopwords = hazm.stopwords_list()\n",
        "\n",
        "# Remove stopwords from the 'article' column\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "dt['article'] = dt['article'].apply(remove_stopwords)\n",
        "\n",
        "# Remove stopwords from the 'summary' column\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "dt['summary'] = dt['summary'].apply(remove_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LIgO1Zr83az7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\METACO\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize the data\n",
        "nltk.download('punkt')  # Download the necessary tokenizer data\n",
        "df['article_tokens'] = df['article'].apply(word_tokenize)\n",
        "df['summary_tokens'] = df['summary'].apply(word_tokenize)\n",
        "df['article_summary']=df['article_tokens']+df['summary_tokens']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JNNHPbG_3axh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similar words to 'مردان':\n",
            "بزرگسالان 0.7595896124839783\n",
            "مردان، 0.7570571899414062\n",
            "دختران 0.745780885219574\n",
            "زنان 0.7314341068267822\n",
            "مدال‌آور 0.6942082643508911\n",
            "باردار 0.6864672899246216\n",
            "آلیش 0.6738450527191162\n",
            "باردار، 0.6624016761779785\n",
            "خانم‌ها 0.6608533263206482\n",
            "مسن 0.6546157598495483\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train a word2vec model\n",
        "model = Word2Vec(df['article_summary'], min_count=1, vector_size=100)  \n",
        "# Change vector_size according to your needs\n",
        "\n",
        "# Test the word2vec model\n",
        "word = 'مردان'  # Replace with the word you want to test\n",
        "similar_words = model.wv.most_similar(word)\n",
        "\n",
        "print(f\"Similar words to '{word}':\")\n",
        "for similar_word, similarity in similar_words:\n",
        "    print(similar_word, similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'split'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Tokenize the article summaries\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokenized_summaries \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39marticle_summary\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m tokenized_summaries \u001b[39m=\u001b[39m [summary\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m summary \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39marticle_summary\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      9\u001b[0m \u001b[39m# Train a Word2Vec model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(sentences\u001b[39m=\u001b[39mtokenized_summaries, min_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, vector_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
            "Cell \u001b[1;32mIn[15], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Tokenize the article summaries\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokenized_summaries \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39marticle_summary\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m tokenized_summaries \u001b[39m=\u001b[39m [summary\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m summary \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39marticle_summary\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      9\u001b[0m \u001b[39m# Train a Word2Vec model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(sentences\u001b[39m=\u001b[39mtokenized_summaries, min_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, vector_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize the article summaries\n",
        "tokenized_summaries = df['article_summary']\n",
        "tokenized_summaries = [summary.split('.') for summary in df['article_summary']]\n",
        "\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_summaries, min_count=1, vector_size=100)\n",
        "# Change vector_size according to your needs\n",
        "\n",
        "# Pad the sequences\n",
        "padded_sequences = pad_sequences(tokenized_summaries)\n",
        "\n",
        "# Test the Word2Vec model\n",
        "word = 'مردان'  # Replace with the word you want to test\n",
        "similar_words = model.wv.most_similar(word)\n",
        "\n",
        "print(f\"Similar words to '{word}':\")\n",
        "for similar_word, similarity in similar_words:\n",
        "    print(similar_word, similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Tokenize the article summaries\n",
        "tokenized_summaries = [summary.split() for summary in df['article_summary']]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_summaries, min_count=1, vector_size=100)\n",
        "# Change vector_size according to your needs\n",
        "\n",
        "# Embed the article summaries\n",
        "embedded_summaries = []\n",
        "for summary in tokenized_summaries:\n",
        "    embedded_summary = [model.wv[word] for word in summary if word in model.wv]\n",
        "    embedded_summaries.append(embedded_summary)\n",
        "\n",
        "# Convert the embedded summaries to numpy array\n",
        "embedded_summaries = np.array(embedded_summaries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GRU, Dense\n",
        "\n",
        "# Placeholder values for max_article_length, max_summary_length, and vocab_size\n",
        "max_article_length = 100\n",
        "max_summary_length = 20\n",
        "vocab_size = 5000\n",
        "\n",
        "# Encoder model\n",
        "encoder_inputs = Input(shape=(max_article_length,))\n",
        "encoder_gru = GRU(units=256)(encoder_inputs)\n",
        "encoder_outputs, state = encoder_gru\n",
        "\n",
        "encoder_states = [state]\n",
        "\n",
        "# Decoder model\n",
        "decoder_inputs = Input(shape=(max_summary_length,))\n",
        "decoder_gru = GRU(units=256, return_sequences=True)(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_outputs = Dense(vocab_size, activation='softmax')(decoder_gru)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Placeholder values for max_article_length, max_summary_length, and vocab_size\n",
        "max_article_length = 100\n",
        "max_summary_length = 20\n",
        "vocab_size = 5000\n",
        "\n",
        "# Encoder model\n",
        "encoder_inputs = Input(shape=(max_article_length,))\n",
        "encoder_lstm = LSTM(units=256)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder model\n",
        "decoder_inputs = Input(shape=(max_summary_length,))\n",
        "decoder_lstm = LSTM(units=256, return_sequences=True)(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_outputs = Dense(vocab_size, activation='softmax')(decoder_lstm)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have prepared your training and validation data\n",
        "train_input_data = ...\n",
        "train_target_data = ...\n",
        "val_input_data = ...\n",
        "val_target_data = ...\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_input_data, train_target_data, validation_data=(val_input_data, val_target_data), epochs=10)\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming you have prepared your training data (train_input_data, train_target_data)\n",
        "model.fit(train_input_data, train_target_data, epochs=10, batch_size=32, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_summaries(predictions, index_to_word):\n",
        "    decoded_summaries = []\n",
        "    for prediction in predictions:\n",
        "        decoded_summary = ' '.join([index_to_word.get(idx, '') for idx in prediction])\n",
        "        decoded_summaries.append(decoded_summary)\n",
        "    return decoded_summaries\n",
        "\n",
        "\n",
        "from rouge import Rouge\n",
        "\n",
        "# Assuming you have prepared your test data (test_input_data, test_target_data)\n",
        "test_input_data = ...\n",
        "test_target_data = ...\n",
        "\n",
        "# Generate summaries from the model\n",
        "predicted_summaries = model.predict(test_input_data)\n",
        "\n",
        "# Convert the predicted summaries and target summaries to strings\n",
        "predicted_summaries = decode_summaries(predicted_summaries)  # Convert numerical sequences to text\n",
        "target_summaries = decode_summaries(test_target_data)\n",
        "\n",
        "# Initialize the Rouge object\n",
        "rouge = Rouge()\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores = rouge.get_scores(predicted_summaries, target_summaries, avg=True)\n",
        "\n",
        "# Print the ROUGE scores\n",
        "print(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
